{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.base_class import BaseAlgorithm\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.off_policy_algorithm import OffPolicyAlgorithm\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch as th\n",
    "\n",
    "# ===================== load the reward module ===================== #\n",
    "from rllte.xplore.reward import RE3\n",
    "# ===================== load the reward module ===================== #\n",
    "\n",
    "class RLeXploreCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A custom callback for the RLeXplore toolkit. \n",
    "    \"\"\"\n",
    "    def __init__(self, irs, verbose=0):\n",
    "        super(RLeXploreCallback, self).__init__(verbose)\n",
    "        self.irs = irs\n",
    "        self.buffer = None\n",
    "\n",
    "    def init_callback(self, model: BaseAlgorithm) -> None:\n",
    "        super().init_callback(model)\n",
    "        if isinstance(self.model, OnPolicyAlgorithm):\n",
    "            self.buffer = self.model.rollout_buffer\n",
    "        # TODO: support for off-policy algorithms will be added soon!!!\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        This method will be called by the model after each call to `env.step()`.\n",
    "\n",
    "        :return: (bool) If the callback returns False, training is aborted early.\n",
    "        \"\"\"\n",
    "        observations = self.locals[\"obs_tensor\"]\n",
    "        device = observations.device\n",
    "        actions = th.as_tensor(self.locals[\"actions\"], device=device)\n",
    "        rewards = th.as_tensor(self.locals[\"rewards\"], device=device)\n",
    "        dones = th.as_tensor(self.locals[\"dones\"], device=device)\n",
    "        next_observations = th.as_tensor(self.locals[\"new_obs\"], device=device)\n",
    "\n",
    "        # ===================== watch the interaction ===================== #\n",
    "        self.irs.watch(observations, actions, rewards, dones, dones, next_observations)\n",
    "        # ===================== watch the interaction ===================== #\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        # ===================== compute the intrinsic rewards ===================== #\n",
    "        obs = th.as_tensor(self.buffer.observations, device=device)\n",
    "        actions = th.as_tensor(self.buffer.actions, device=device)\n",
    "        rewards = th.as_tensor(self.buffer.rewards, device=device)\n",
    "        dones = th.as_tensor(self.buffer.episode_starts, device=device)\n",
    "        print(obs.shape, actions.shape, rewards.shape, dones.shape, obs.shape)\n",
    "        intrinsic_rewards = irs.compute(samples=dict(observations=obs, \n",
    "                                                     actions=actions, \n",
    "                                                     rewards=rewards, \n",
    "                                                     terminateds=dones,\n",
    "                                                     truncateds=dones, \n",
    "                                                     next_observations=obs\n",
    "                                                     ))\n",
    "        self.buffer.advantages += intrinsic_rewards.cpu().numpy()\n",
    "        self.buffer.returns += intrinsic_rewards.cpu().numpy()\n",
    "        # ===================== compute the intrinsic rewards ===================== #\n",
    "\n",
    "# Parallel environments\n",
    "device = 'cuda'\n",
    "n_envs = 4\n",
    "envs = make_vec_env(\"Pendulum-v1\", n_envs=n_envs)\n",
    "\n",
    "# ===================== build the reward ===================== #\n",
    "irs = RE3(envs, device=device)\n",
    "# ===================== build the reward ===================== #\n",
    "\n",
    "model = PPO(\"MlpPolicy\", envs, verbose=1, device=device)\n",
    "model.learn(total_timesteps=25000, callback=RLeXploreCallback(irs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllte",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
